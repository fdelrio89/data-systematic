{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7151d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:37:38.072327: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-8.0/lib64/\n",
      "2024-05-16 14:37:38.072364: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1392f86d1a743b188e420b8aa38b672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a558475673b4743a9841015be086e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21849d1103594df48de83e17acbc10a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:01, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fidelrio/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"/home/fidelrio/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/home/fidelrio/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fidelrio/.pyenv/versions/3.7.5/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/fidelrio/.pyenv/versions/3.7.5/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/fidelrio/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">50</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>images = images.to(device)                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">48 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>all_outs.append(vit(images).detach().cpu())                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>50 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>all_feats = torch.cat(all_outs)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">51 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>image_name_to_idx = {image_path.name: idx <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> idx, image_path <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(dataset.i    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">53 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NotImplementedError: </span>There were no tensor arguments to this function <span style=\"font-weight: bold\">(</span>e.g., you passed an empty list of Tensors<span style=\"font-weight: bold\">)</span>, \n",
       "but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a \n",
       "non-empty list of Tensors, or that you <span style=\"font-weight: bold\">(</span>the operator writer<span style=\"font-weight: bold\">)</span> forgot to register a fallback function.  Available \n",
       "functions are <span style=\"font-weight: bold\">[</span>CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, \n",
       "ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, \n",
       "AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, \n",
       "AutocastCPU, Autocast, Batched, VmapMode, Functionalize<span style=\"font-weight: bold\">]</span>.\n",
       "\n",
       "CPU: registered at aten/src/ATen/RegisterCPU.cpp:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21063</span> <span style=\"font-weight: bold\">[</span>kernel<span style=\"font-weight: bold\">]</span>\n",
       "CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29726</span> <span style=\"font-weight: bold\">[</span>kernel<span style=\"font-weight: bold\">]</span>\n",
       "QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1258</span> <span style=\"font-weight: bold\">[</span>kernel<span style=\"font-weight: bold\">]</span>\n",
       "BackendSelect: fallthrough registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/core/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">BackendSelectFallbackKernel.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Python: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/core/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">PythonFallbackKernel.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Named: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/core/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">NamedRegistrations.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Conjugate: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ConjugateFallback.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Negative: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/native/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">NegateFallback.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "ZeroTensor: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ZeroTensorFallback.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">86</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "ADInplaceOrView: fallthrough registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/core/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableFallbackKernel.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "AutogradOther: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradCPU: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradCUDA: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradXLA: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradLazy: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradXPU: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradMLC: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradHPU: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradNestedTensor: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradPrivateUse1: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradPrivateUse2: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradPrivateUse3: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11380</span> <span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "Tracer: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">TraceType_3.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11220</span> <span style=\"font-weight: bold\">[</span>kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutocastCPU: fallthrough registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">autocast_mode.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">461</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Autocast: fallthrough registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">autocast_mode.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">305</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Batched: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">BatchingRegistrations.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1059</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "VmapMode: fallthrough registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VmapModeRegistrations.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Functionalize: registered at ..<span style=\"color: #800080; text-decoration-color: #800080\">/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">FunctionalizeFallbackKernel.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m50\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m47 \u001b[0m\u001b[2m│   │   \u001b[0mimages = images.to(device)                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m48 \u001b[0m\u001b[2m│   │   \u001b[0mall_outs.append(vit(images).detach().cpu())                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m49 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m50 \u001b[2m│   \u001b[0mall_feats = torch.cat(all_outs)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m51 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m52 \u001b[0m\u001b[2m│   \u001b[0mimage_name_to_idx = {image_path.name: idx \u001b[94mfor\u001b[0m idx, image_path \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(dataset.i    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m53 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNotImplementedError: \u001b[0mThere were no tensor arguments to this function \u001b[1m(\u001b[0me.g., you passed an empty list of Tensors\u001b[1m)\u001b[0m, \n",
       "but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a \n",
       "non-empty list of Tensors, or that you \u001b[1m(\u001b[0mthe operator writer\u001b[1m)\u001b[0m forgot to register a fallback function.  Available \n",
       "functions are \u001b[1m[\u001b[0mCPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, \n",
       "ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, \n",
       "AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, \n",
       "AutocastCPU, Autocast, Batched, VmapMode, Functionalize\u001b[1m]\u001b[0m.\n",
       "\n",
       "CPU: registered at aten/src/ATen/RegisterCPU.cpp:\u001b[1;36m21063\u001b[0m \u001b[1m[\u001b[0mkernel\u001b[1m]\u001b[0m\n",
       "CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:\u001b[1;36m29726\u001b[0m \u001b[1m[\u001b[0mkernel\u001b[1m]\u001b[0m\n",
       "QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:\u001b[1;36m1258\u001b[0m \u001b[1m[\u001b[0mkernel\u001b[1m]\u001b[0m\n",
       "BackendSelect: fallthrough registered at ..\u001b[35m/aten/src/ATen/core/\u001b[0m\u001b[95mBackendSelectFallbackKernel.cpp\u001b[0m:\u001b[1;36m3\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Python: registered at ..\u001b[35m/aten/src/ATen/core/\u001b[0m\u001b[95mPythonFallbackKernel.cpp\u001b[0m:\u001b[1;36m47\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Named: registered at ..\u001b[35m/aten/src/ATen/core/\u001b[0m\u001b[95mNamedRegistrations.cpp\u001b[0m:\u001b[1;36m7\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Conjugate: registered at ..\u001b[35m/aten/src/ATen/\u001b[0m\u001b[95mConjugateFallback.cpp\u001b[0m:\u001b[1;36m18\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Negative: registered at ..\u001b[35m/aten/src/ATen/native/\u001b[0m\u001b[95mNegateFallback.cpp\u001b[0m:\u001b[1;36m18\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "ZeroTensor: registered at ..\u001b[35m/aten/src/ATen/\u001b[0m\u001b[95mZeroTensorFallback.cpp\u001b[0m:\u001b[1;36m86\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "ADInplaceOrView: fallthrough registered at ..\u001b[35m/aten/src/ATen/core/\u001b[0m\u001b[95mVariableFallbackKernel.cpp\u001b[0m:\u001b[1;36m64\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "AutogradOther: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradCPU: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradCUDA: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradXLA: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradLazy: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradXPU: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradMLC: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradHPU: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradNestedTensor: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradPrivateUse1: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradPrivateUse2: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradPrivateUse3: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_3.cpp\u001b[0m:\u001b[1;36m11380\u001b[0m \u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "Tracer: registered at ..\u001b[35m/torch/csrc/autograd/generated/\u001b[0m\u001b[95mTraceType_3.cpp\u001b[0m:\u001b[1;36m11220\u001b[0m \u001b[1m[\u001b[0mkernel\u001b[1m]\u001b[0m\n",
       "AutocastCPU: fallthrough registered at ..\u001b[35m/aten/src/ATen/\u001b[0m\u001b[95mautocast_mode.cpp\u001b[0m:\u001b[1;36m461\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Autocast: fallthrough registered at ..\u001b[35m/aten/src/ATen/\u001b[0m\u001b[95mautocast_mode.cpp\u001b[0m:\u001b[1;36m305\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Batched: registered at ..\u001b[35m/aten/src/ATen/\u001b[0m\u001b[95mBatchingRegistrations.cpp\u001b[0m:\u001b[1;36m1059\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "VmapMode: fallthrough registered at ..\u001b[35m/aten/src/ATen/\u001b[0m\u001b[95mVmapModeRegistrations.cpp\u001b[0m:\u001b[1;36m33\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Functionalize: registered at ..\u001b[35m/aten/src/ATen/\u001b[0m\u001b[95mFunctionalizeFallbackKernel.cpp\u001b[0m:\u001b[1;36m52\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from model import ViTEmbedding\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ImageDataset:\n",
    "    def __init__(self, images_dir, split):\n",
    "        self.images_dir = images_dir\n",
    "        self.image_list = list(Path(images_dir).glob('*.png'))\n",
    "        self.image_list = sorted(self.image_list,\n",
    "                                 key=lambda x: int(x.name.replace('.png','').replace(f'CLEVR_{split}_','')))\n",
    "        self.transforms = transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_list[idx]\n",
    "        image_tensor = self.transforms(self.load_image(image_path))\n",
    "        return image_tensor\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        return Image.open(image_path).convert('RGB')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "base_dir = '/workspace1/fidelrio/CLEVR_CoGenT_v1.0/'\n",
    "base_dir2 = '/mnt/ialabnas/datasets/CLEVR_CoGenT_v1.0/'\n",
    "    \n",
    "device = torch.device('cuda')\n",
    "\n",
    "vit = ViTEmbedding().to(device)\n",
    "\n",
    "# for split in ['trainA', 'valA', 'valB', 'testA', 'testB']:\n",
    "for split in ['valA', 'valB', 'testA', 'testB']:\n",
    "    images_dir = f'{base_dir}/images/{split}'\n",
    "    dataset = ImageDataset(images_dir, split)\n",
    "\n",
    "    num_workers = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", 4))\n",
    "    loader = DataLoader(dataset, batch_size=32, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    all_outs = []\n",
    "    for images in tqdm(loader):\n",
    "        images = images.to(device)\n",
    "        all_outs.append(vit(images).detach().cpu())\n",
    "\n",
    "    all_feats = torch.cat(all_outs)\n",
    "\n",
    "    image_name_to_idx = {image_path.name: idx for idx, image_path in enumerate(dataset.image_list)}\n",
    "\n",
    "    torch.save({\n",
    "            'image_name_to_idx': image_name_to_idx,\n",
    "            'image_features': all_feats,\n",
    "        },\n",
    "        f'{base_dir}/images/{split}-vit.pt')\n",
    "    \n",
    "    torch.save({\n",
    "            'image_name_to_idx': image_name_to_idx,\n",
    "            'image_features': all_feats,\n",
    "        },\n",
    "        f'{base_dir2}/images/{split}-vit.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188b326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

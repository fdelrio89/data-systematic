{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc18d515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 09:41:59.991876: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-8.0/lib64/\n",
      "2024-09-26 09:41:59.991906: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/mnt/ialabnas/homes/fidelrio/systematic-text-representations/')\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from config import load_config\n",
    "from data import build_datasets, build_loader, build_detailed_test_dataloaders\n",
    "from data import CollatorForMaskedSelectedTokens, CollatorForMaskedRandomSelectedTokens, IdentityCollator\n",
    "from data import ALL_POSSIBLE_COLORS\n",
    "from model import MultimodalModel, MultimodalPretrainingModel\n",
    "from utils import load_checkpoint\n",
    "from lightning import Trainer, seed_everything\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0608ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b47f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scene_tensor_to_txt(tensor):\n",
    "    return ' '.join([processor.inv_vocabulary[t] for t in tensor.tolist()])\n",
    "\n",
    "def print_scene_tensor(tensor):\n",
    "    scene_text = scene_tensor_to_txt(tensor)\n",
    "    print(scene_text.replace('[PAD]', '').replace('[SEP]','\\n     '))\n",
    "    \n",
    "def print_parallel(tensor0, tensor1, tensor2, confidences, titles):\n",
    "    ttl0, ttl1, ttl2 = titles\n",
    "    print(f'{ttl0:6.6s} {ttl1:6.6s} {ttl2:6.6s}')\n",
    "    for t0, t1, t2, conf in zip(\n",
    "            tensor0.tolist(), tensor1.tolist(), tensor2.tolist(), confidences.tolist()):\n",
    "        w0 = processor.inv_vocabulary[t0]\n",
    "        w1 = processor.inv_vocabulary[t1]\n",
    "        w2 = processor.inv_vocabulary[t2]\n",
    "        \n",
    "        if w0 == '[SEP]':\n",
    "            print()\n",
    "            continue\n",
    "        if w0 == '[PAD]':\n",
    "            break\n",
    "        \n",
    "        print_txt = f'{w0:6.6s} {w1:6.6s} {w2:6.6s} ({conf:.4f})'\n",
    "        if w0 != w2:\n",
    "            print_txt = bold(print_txt)\n",
    "            \n",
    "\n",
    "        print(print_txt)\n",
    "        \n",
    "def bold(text):\n",
    "    return (\"\\033[1m\" + text + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbf4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a64e591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 999\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "n_colors = 8\n",
    "epoch = None\n",
    "exp_name = f'mmlm--n_colors={n_colors}c--mlm_probability=0.15'\n",
    "\n",
    "checkpoint = load_checkpoint(exp_name, epoch=epoch)\n",
    "print('Epoch:', checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56aca98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92f3603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'epoch=149-step=22050.ckpt'  'epoch=649-step=95550.ckpt'\r\n",
      "'epoch=199-step=29400.ckpt'  'epoch=699-step=102900.ckpt'\r\n",
      "'epoch=19-step=2940.ckpt'    'epoch=69-step=10290.ckpt'\r\n",
      "'epoch=249-step=36750.ckpt'  'epoch=749-step=110250.ckpt'\r\n",
      "'epoch=299-step=44100.ckpt'  'epoch=799-step=117600.ckpt'\r\n",
      "'epoch=29-step=4410.ckpt'    'epoch=79-step=11760.ckpt'\r\n",
      "'epoch=349-step=51450.ckpt'  'epoch=849-step=124950.ckpt'\r\n",
      "'epoch=399-step=58800.ckpt'  'epoch=899-step=132300.ckpt'\r\n",
      "'epoch=39-step=5880.ckpt'    'epoch=89-step=13230.ckpt'\r\n",
      "'epoch=449-step=66150.ckpt'  'epoch=949-step=139650.ckpt'\r\n",
      "'epoch=499-step=73500.ckpt'  'epoch=999-step=147000.ckpt'\r\n",
      "'epoch=49-step=7350.ckpt'    'epoch=99-step=14700.ckpt'\r\n",
      "'epoch=549-step=80850.ckpt'  'epoch=9-step=1470.ckpt'\r\n",
      "'epoch=599-step=88200.ckpt'   last.ckpt\r\n",
      "'epoch=59-step=8820.ckpt'\r\n"
     ]
    }
   ],
   "source": [
    "!ls outputs/$exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1336bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mmlm--n_colors=8c--mlm_probability=0.15 last checkpoint config from outputs/mmlm--n_colors=8c--mlm_probability=0.15/last.ckpt\n",
      "Add new arg: permute_pixels = False\n",
      "Add new arg: aug_zero_color = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fidelrio/.pyenv/versions/systematicity/lib/python3.7/site-packages/lightning/pytorch/utilities/parsing.py:270: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "  f\"Attribute {k!r} is an instance of `nn.Module` and is already saved during checkpointing.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config(exp_name)\n",
    "\n",
    "config.vocabulary_path = config.vocabulary_path.replace('/workspace/' ,'/workspace1/')\n",
    "config.base_path = config.base_path.replace('/workspace/' ,'/workspace1/')\n",
    "\n",
    "# pp.pprint(vars(config))\n",
    "\n",
    "train_dataset, test_dataset, systematic_dataset, common_systematic_dataset = build_datasets(config)\n",
    "config.pad_idx = train_dataset.pad_idx\n",
    "config.n_tokens = train_dataset.n_tokens\n",
    "\n",
    "complete_dataset = ConcatDataset((test_dataset, systematic_dataset))\n",
    "\n",
    "processor = test_dataset.processor\n",
    "mask_token_idx = processor.vocabulary['[MASK]']\n",
    "pad_token_idx = processor.vocabulary['[PAD]']\n",
    "\n",
    "# test_loaders = build_detailed_test_dataloaders(test_dataset, config) # type_of_tokens_to_test\n",
    "# systematic_loaders = build_detailed_test_dataloaders(systematic_dataset, config) # type_of_tokens_to_test\n",
    "\n",
    "model = MultimodalModel(config).to(device)\n",
    "training_model = MultimodalPretrainingModel(model, config).to(device)\n",
    "training_model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be2d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24c802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = processor.vocabulary\n",
    "\n",
    "relation_tokens = sorted(\n",
    "    [vocab[w] for w in ['left', 'right', 'behind', 'front'] if w in vocab])\n",
    "colors_tokens = sorted(\n",
    "    [vocab[w] for w in ALL_POSSIBLE_COLORS if w in vocab])\n",
    "#     [vocab[w] for w in ['blue', 'brown', 'cyan', 'green', 'red', 'purple', 'yellow', 'gray']])\n",
    "shapes_tokens = sorted(\n",
    "    [vocab[w] for w in ['cylinder', 'sphere', 'cube'] if w in vocab])\n",
    "materials_tokens = sorted(\n",
    "    [vocab[w] for w in ['metal', 'rubber'] if w in vocab])\n",
    "sizes_tokens = sorted(\n",
    "    [vocab[w] for w in ['small', 'large'] if w in vocab])\n",
    "\n",
    "random_baseline = {\n",
    "    'relation':  1 / len(relation_tokens),\n",
    "    'color':  1 / len(colors_tokens),\n",
    "    'shapes':  1 / len(shapes_tokens),\n",
    "    'materials':  1 / len(materials_tokens),\n",
    "    'size':  1 / len(sizes_tokens),\n",
    "    'identity':  1 / len(processor.vocabulary),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d310a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8316f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fidelrio/.pyenv/versions/systematicity/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "test_indices = random.sample(range(len(test_dataset)), k=batch_size)\n",
    "pc_subset_test = Subset(test_dataset, test_indices)\n",
    "pc_subset_systematic = Subset(systematic_dataset, test_indices)\n",
    "pc_subset_complete = Subset(complete_dataset, test_indices)\n",
    "\n",
    "colors_collator = CollatorForMaskedSelectedTokens(config, processor, tokens=colors_tokens)\n",
    "shapes_collator = CollatorForMaskedSelectedTokens(config, processor, tokens=shapes_tokens)\n",
    "materials_collator = CollatorForMaskedSelectedTokens(config, processor, tokens=materials_tokens)\n",
    "sizes_collator = CollatorForMaskedSelectedTokens(config, processor, tokens=sizes_tokens)\n",
    "dlkwargs = {\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': int(os.environ.get(\"SLURM_CPUS_PER_TASK\", 4)),\n",
    "    'pin_memory': torch.cuda.is_available(),\n",
    "}\n",
    "\n",
    "test_loaders = {}\n",
    "systematic_loaders = {}\n",
    "complete_loaders = {}\n",
    "for task, collator in [('colors', colors_collator),\n",
    "                       ('shapes', shapes_collator),\n",
    "                       ('materials', materials_collator),\n",
    "                       ('sizes', sizes_collator)]:\n",
    "    \n",
    "    test_loaders[task] = DataLoader(\n",
    "        pc_subset_test, shuffle=True, collate_fn=collator, **dlkwargs)\n",
    "    systematic_loaders[task] = DataLoader(\n",
    "        pc_subset_systematic, shuffle=True, collate_fn=collator, **dlkwargs)\n",
    "    complete_loaders[task] = DataLoader(\n",
    "        pc_subset_complete, shuffle=True, collate_fn=collator, **dlkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593821b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f5dc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f207b2bb490>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps = []  # This will be a list of Tensors, each representing a feature map\n",
    "\n",
    "def hook_feat_map(mod, inp, out):\n",
    "    feature_maps.clear()\n",
    "    feature_maps.append(out)\n",
    "\n",
    "model.transformer.register_forward_hook(hook_feat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99105f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a33f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_props(scenes):\n",
    "    sizes = scenes[:,1:][:,0::5]\n",
    "    colors = scenes[:,1:][:,1::5]\n",
    "    materials = scenes[:,1:][:,2::5]\n",
    "    shapes = scenes[:,1:][:,3::5]\n",
    "\n",
    "    sizes = sizes[sizes != pad_token_idx].cpu()\n",
    "    colors = colors[colors != pad_token_idx].cpu()\n",
    "    materials = materials[materials != pad_token_idx].cpu()\n",
    "    shapes = shapes[shapes != pad_token_idx].cpu()\n",
    "    return sizes, colors, materials, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b23d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks = ['colors', 'shapes', 'materials', 'sizes']\n",
    "\n",
    "# feats_by_set = {}\n",
    "# gt_by_set = {}\n",
    "# props_by_set = {}\n",
    "# for test_name, loader in [('test', test_loaders), ('systematic', systematic_loaders)]:\n",
    "#     feats_by_task = {}\n",
    "#     gt_by_task = {}\n",
    "#     props_by_task = {}\n",
    "#     for task in tasks:\n",
    "#         images, scenes, labels = next(iter(loader[task]))\n",
    "#         images, scenes, labels = images.to(device), scenes.to(device), labels.to(device)\n",
    "#         cimages, cscenes, clabels = images, scenes, labels\n",
    "#         with torch.no_grad():\n",
    "#             output_logits = model(images, scenes)\n",
    "\n",
    "#             features = feature_maps[0]\n",
    "#             confidences = softmax(output_logits, dim=-1).max(dim=-1).values\n",
    "#             predictions = output_logits.argmax(dim=-1)\n",
    "\n",
    "#             scene_features = features.transpose(1,0)[:,-config.max_scene_size:]\n",
    "#             mask_idxs = (scenes == mask_token_idx)\n",
    "#             gt_by_task[task] = labels[:,-config.max_scene_size:][mask_idxs].cpu()\n",
    "#             feats_by_task[task] = scene_features[mask_idxs].cpu()\n",
    "#             props_by_task[task] = get_props(scenes)\n",
    "#             sizes, colors, materials, shapes = props_by_task[task]\n",
    "   \n",
    "#     feats_by_set[test_name] = feats_by_task\n",
    "#     gt_by_set[test_name] = gt_by_task\n",
    "#     props_by_set[test_name] = props_by_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c0eaf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_idxs = torch.unique(torch.cat([gt_by_set['test'][t] for t in tasks]))\n",
    "# clf_idxs_by_task = {t: torch.unique(gt_by_set['test'][t]) for t in tasks}\n",
    "# all_clf_vectors = model.classifier.weight.data.cpu()\n",
    "# clf_vectors = model.classifier.weight.data.cpu()[clf_idxs]\n",
    "# clf_vectors_by_task = {t: all_clf_vectors[tidxs] for t, tidxs in clf_idxs_by_task.items()}\n",
    "# all_word_vectors = model.word_embedding.weight.data.cpu()\n",
    "# word_vectors = model.word_embedding.weight.data.cpu()[clf_idxs]\n",
    "# word_vectors_by_task = {t: all_clf_vectors[tidxs] for t, tidxs in clf_idxs_by_task.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e8bffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test_name, loader in [\n",
    "#         ('test', test_loaders), ('systematic', systematic_loaders), ('complete', complete_loaders)]:\n",
    "#     for task in tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad1a779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'shapes'\n",
    "loader = complete_loaders\n",
    "\n",
    "task_idx = ['sizes', 'colors', 'materials', 'shapes'].index(task)\n",
    "\n",
    "images, scenes, labels = next(iter(loader[task]))\n",
    "images, scenes, labels = images.to(device), scenes.to(device), labels.to(device)\n",
    "with torch.no_grad():\n",
    "    output_logits = model(images, scenes)\n",
    "\n",
    "    features = feature_maps[0]\n",
    "    confidences = softmax(output_logits, dim=-1).max(dim=-1).values\n",
    "    predictions = output_logits.argmax(dim=-1)\n",
    "\n",
    "    scene_features = features.transpose(1,0)[:,-config.max_scene_size:]\n",
    "    mask_idxs = (scenes == mask_token_idx)\n",
    "    gt = labels[:,-config.max_scene_size:][mask_idxs].cpu()\n",
    "    feats = scene_features[mask_idxs].cpu()\n",
    "    props = get_props(scenes)\n",
    "    sizes, colors, materials, shapes = props\n",
    "\n",
    "\n",
    "\n",
    "prop_stack = [sizes, colors, materials, shapes]\n",
    "prop_stack[task_idx] = gt\n",
    "prop_stack = torch.stack(prop_stack, dim=-1).tolist()\n",
    "prop_index = defaultdict(list)\n",
    "for idx, props in enumerate(prop_stack):\n",
    "    props = tuple(props)\n",
    "    prop_index[props].append(idx)\n",
    "\n",
    "    \n",
    "unique_sizes = sizes.unique().tolist()\n",
    "unique_colors = colors.unique().tolist()\n",
    "unique_materials = materials.unique().tolist()\n",
    "unique_shapes = shapes.unique().tolist()\n",
    "unique_tokens = [\n",
    "    unique_sizes,\n",
    "    unique_colors,\n",
    "    unique_materials,\n",
    "    unique_shapes,\n",
    "]\n",
    "\n",
    "unique_gts = gt.unique().tolist()\n",
    "unique_tokens[task_idx] = unique_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d82468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9b5772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "\n",
    "def all_possible_dichotomies(unique_tokens, dichotomy_within_task_idx=None):\n",
    "    dichotomies = set()\n",
    "    for dichotomic_tokens_idx in range(len(unique_tokens)):\n",
    "        if dichotomy_within_task_idx is not None:\n",
    "            if dichotomic_tokens_idx != dichotomy_within_task_idx:\n",
    "                continue\n",
    "\n",
    "        context_tokens = unique_tokens[:dichotomic_tokens_idx] + unique_tokens[dichotomic_tokens_idx+1:]\n",
    "        dichotomic_tokens = unique_tokens[dichotomic_tokens_idx]\n",
    "        for dichotomic_pair in combinations(dichotomic_tokens, r=2):\n",
    "            for pair in [dichotomic_pair, tuple(reversed(dichotomic_pair))]:\n",
    "                for context0 in product(*context_tokens):\n",
    "                    for context1 in product(*context_tokens):\n",
    "                        if context0 == context1:\n",
    "                            continue\n",
    "\n",
    "                        a, b = pair\n",
    "                        if a == b:\n",
    "                            continue\n",
    "                        from_v0 = list(context0).copy()\n",
    "                        from_v0.insert(dichotomic_tokens_idx, a)\n",
    "                        from_v1 = list(context1).copy()\n",
    "                        from_v1.insert(dichotomic_tokens_idx, a)\n",
    "                        to_v0 = list(context0).copy()\n",
    "                        to_v0.insert(dichotomic_tokens_idx, b)\n",
    "                        to_v1 = list(context1).copy()\n",
    "                        to_v1.insert(dichotomic_tokens_idx, b)\n",
    "\n",
    "                        dichotomies.add((tuple(from_v0),\n",
    "                                            tuple(from_v1),\n",
    "                                            tuple(to_v0),\n",
    "                                            tuple(to_v1)))\n",
    "\n",
    "    return list(dichotomies)\n",
    "\n",
    "\n",
    "def is_possible(from_v0, from_v1, to_v0, to_v1, prop_index):\n",
    "    if not prop_index[from_v0]:\n",
    "        return False\n",
    "    if not prop_index[from_v1]:\n",
    "        return False\n",
    "    if not prop_index[to_v0]:\n",
    "        return False\n",
    "    if not prop_index[to_v1]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def all_possible_triplets(unique_tokens, only_task_mod_ids=None):\n",
    "    only_task_mod_idx1 = only_task_mod_idx2 = None\n",
    "    if only_task_mod_ids is not None:\n",
    "        only_task_mod_idx1, only_task_mod_idx2 = only_task_mod_ids\n",
    "        \n",
    "    task_ids = list(range(len(unique_tokens)))\n",
    "    \n",
    "    triplets = set()\n",
    "    context_tokens = unique_tokens.copy()\n",
    "    for context in product(*context_tokens):\n",
    "        for task_mod_idx1, task_mod_idx2 in combinations(task_ids, r=2):\n",
    "            if task_mod_idx1 == task_mod_idx2:\n",
    "                    continue\n",
    "\n",
    "            if only_task_mod_ids is not None:\n",
    "                if only_task_mod_idx1 is not None and task_mod_idx1 != only_task_mod_idx1: \n",
    "                    continue\n",
    "                if only_task_mod_idx2 is not None and task_mod_idx2 != only_task_mod_idx2:\n",
    "                    continue\n",
    "                    \n",
    "            tokens_mod1 = unique_tokens[task_mod_idx1]\n",
    "            tokens_mod2 = unique_tokens[task_mod_idx2]\n",
    "            for token_mod_idx1, token_mod_idx2 in product(tokens_mod1, tokens_mod2):\n",
    "                if context[task_mod_idx1] == token_mod_idx1:\n",
    "                    continue\n",
    "                if context[task_mod_idx2] == token_mod_idx2:\n",
    "                    continue\n",
    "                \n",
    "                pivot = list(context).copy()\n",
    "                v1 = list(context).copy()\n",
    "                v1[task_mod_idx1] = token_mod_idx1\n",
    "                v2 = list(context).copy()\n",
    "                v2[task_mod_idx2] = token_mod_idx2\n",
    "                triplets.add((tuple(pivot),\n",
    "                              tuple(v1),\n",
    "                              tuple(v2)))\n",
    "    return list(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c33a06d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = all_possible_triplets(unique_tokens, only_task_mod_ids=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a947221f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1344"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff3d19f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((83, 20, 66, 84), (83, 21, 66, 84), (83, 20, 66, 42))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ff839a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59, 17, 77, 42), (59, 19, 77, 42), (59, 17, 77, 40))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be8ea00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((83, 18, 77, 42), (83, 21, 77, 42), (83, 18, 77, 84))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43b5754e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59, 19, 77, 42), (59, 18, 77, 42), (59, 19, 66, 42))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0ac33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf537282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5d6d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267, 291)\n",
      "(291, 267)\n"
     ]
    }
   ],
   "source": [
    "dichotomies = all_possible_dichotomies(unique_tokens, only_task_idx=0)\n",
    "possible_dichotomies = [p for p in dichotomies if is_possible(*p, prop_index=prop_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76b5738b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3356640"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dichotomies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e52e8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 164, 274, 292) -> (267, 164, 274, 292)\n",
      "(291, 67, 285, 292) -> (267, 67, 285, 292)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def print_dichotomy(dichotomy):\n",
    "    from_v0, from_v1, to_v0, to_v1 = dichotomy\n",
    "    print(from_v0, '->', to_v0)\n",
    "    print(from_v1, '->', to_v1)\n",
    "\n",
    "print_dichotomy(random.choice(dichotomies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6893ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6cc65a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64fa464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f505eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf61c3fb7ac4c8aa04414de146f8a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_samples = 20\n",
    "n_sampled_dichotomies = 3500\n",
    "\n",
    "if n_sampled_dichotomies < len(possible_dichotomies):\n",
    "    sampled_dichotomies = random.sample(possible_dichotomies, k=n_sampled_dichotomies)\n",
    "else:\n",
    "    sampled_dichotomies = possible_dichotomies\n",
    "\n",
    "p_scores = []\n",
    "for from_v0, from_v1, to_v0, to_v1 in tqdm(sampled_dichotomies):\n",
    "    for _ in range(n_samples):\n",
    "        from_v0_idx = random.choice(prop_index[from_v0])\n",
    "        from_v1_idx = random.choice(prop_index[from_v1])\n",
    "        to_v0_idx = random.choice(prop_index[to_v0])\n",
    "        to_v1_idx = random.choice(prop_index[to_v1])\n",
    "\n",
    "        vec0 = feats[to_v0_idx] - feats[from_v0_idx]\n",
    "        vec1 = feats[to_v1_idx] - feats[from_v1_idx]\n",
    "\n",
    "        p_score = vec0 @ vec1 / (vec0.norm() * vec1.norm())\n",
    "        p_scores.append(float(p_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a4f0a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000 scores computed\n",
      "2592 property combinations\n",
      "8658576 parallelograms\n",
      "\n",
      "Mean Parallelism Score: 0.2756\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(p_scores)} scores computed')\n",
    "print(f'{len(prop_index)} property combinations')\n",
    "print(f'{len(dichotomies)} parallelograms')\n",
    "print()\n",
    "print(f'Mean Parallelism Score: {float(torch.tensor(p_scores).mean()):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c59aa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p_scores, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ddd43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640148e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "027ecfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_parallelogram(p1, p2, p3, p4):\n",
    "#     # Function to calculate vector between two points\n",
    "#     def vector(a, b):\n",
    "#         return (b[0] - a[0], b[1] - a[1])\n",
    "\n",
    "#     # Function to check if two vectors are equal\n",
    "#     def are_vectors_equal(v1, v2):\n",
    "#         return v1 == v2\n",
    "\n",
    "#     # Get the vectors for opposite sides\n",
    "#     v1 = vector(p1, p2)  # Vector from p1 to p2\n",
    "#     v2 = vector(p3, p4)  # Vector from p3 to p4\n",
    "#     v3 = vector(p1, p4)  # Vector from p1 to p4\n",
    "#     v4 = vector(p2, p3)  # Vector from p2 to p3\n",
    "\n",
    "#     # Opposite sides should have equal vectors for it to be a parallelogram\n",
    "#     return are_vectors_equal(v1, v2) and are_vectors_equal(v3, v4)\n",
    "\n",
    "# all(is_parallelogram(*p) for p in parallelograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48a523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59040b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367219ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "636b2d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 \u001b[94massert\u001b[0m \u001b[94mFalse\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAssertionError\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b65ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_pca(X,\n",
    "                y, \n",
    "                title='', \n",
    "                special_X=None, \n",
    "                special_y=None, \n",
    "                don_t_label_these=[], \n",
    "                labels_to_use=[], \n",
    "                special_labels_to_use=[], \n",
    "                ax=None):\n",
    "    \n",
    "    is_3d = X.shape[-1] == 3\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(9*1.75,5*1.75))\n",
    "        if is_3d:\n",
    "            ax = fig.add_subplot(projection='3d')\n",
    "        else:\n",
    "            ax = fig.add_subplot()\n",
    "\n",
    "    label_namer = processor.inv_vocabulary\n",
    "    if labels_to_use:\n",
    "        label_namer = labels_to_use\n",
    "    for label_idx in sorted(set(y)):\n",
    "        idxs = y == label_idx \n",
    "        label = label_namer[label_idx]\n",
    "\n",
    "        plot_args = [X[:,0][idxs], X[:,1][idxs]]\n",
    "        if is_3d:\n",
    "            plot_args = plot_args + [X[:,2][idxs]]\n",
    "\n",
    "        plot_kwargs = {}\n",
    "        if not don_t_label_these or label not in don_t_label_these:\n",
    "            plot_kwargs['label'] = label\n",
    "\n",
    "        scatter_shapes = ax.scatter(*plot_args, **plot_kwargs)\n",
    "  \n",
    "    special_label_namer = processor.inv_vocabulary\n",
    "    if labels_to_use:\n",
    "        special_label_namer = labels_to_use\n",
    "    if special_labels_to_use:\n",
    "        special_label_namer = special_labels_to_use\n",
    "    if special_X is not None:\n",
    "        for label_idx in sorted(set(special_y)): \n",
    "            label = special_label_namer[label_idx]\n",
    "\n",
    "            plot_kwargs = {}\n",
    "            if not don_t_label_these or label not in don_t_label_these:\n",
    "                plot_kwargs['label'] = label\n",
    "\n",
    "            special_idxs = special_y == label_idx \n",
    "            special_plot_args = [special_X[:,0][special_idxs], special_X[:,1][special_idxs]]\n",
    "            if is_3d:\n",
    "                special_plot_args = special_plot_args + [special_X[:,2][special_idxs]]\n",
    "\n",
    "#             color = scatter_shapes.get_facecolors()[0]\n",
    "#             plot_kwargs['color'] = color\n",
    "            plot_kwargs['marker'] = '*'\n",
    "            if not is_3d:\n",
    "                plot_kwargs['s'] = 200\n",
    "        \n",
    "            ax.scatter(*special_plot_args, **plot_kwargs)\n",
    "\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.legend(framealpha=1, loc='upper left')\n",
    "\n",
    "    # Show plot\n",
    "    # plt.savefig('exports/base-attributes.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
    "    if not ax:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2ccff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17817015",
   "metadata": {},
   "outputs": [],
   "source": [
    "don_t_label_these = [] if n_colors <= 27 else ALL_POSSIBLE_COLORS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d17fe2c",
   "metadata": {},
   "source": [
    "### All Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc711e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be544241",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "labels_to_use = tasks\n",
    "\n",
    "X = torch.cat([feats_by_set['test'][t] for t in tasks]).numpy()\n",
    "all_gts = [torch.full_like(gt_by_set['test'][t], tidx) for tidx, t in enumerate(tasks)]\n",
    "all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "special_X = torch.cat([clf_vectors_by_task[t] for t in tasks]).numpy()\n",
    "special_gts = [torch.full_like(clf_idxs_by_task[t], tidx) for tidx, t in enumerate(tasks)]\n",
    "special_gts = torch.cat(special_gts).numpy()\n",
    "X_2d_clf = pca.transform(clf_vectors)\n",
    "# X_2d_clf = pca.transform(word_vectors)\n",
    "\n",
    "scatter_pca(X_2d,\n",
    "            all_gts,\n",
    "            labels_to_use=labels_to_use,\n",
    "            special_X=X_2d_clf,\n",
    "            special_y=special_gts,\n",
    "            title='IID Test',\n",
    "            don_t_label_these=don_t_label_these,\n",
    "            ax=axs[0])\n",
    "\n",
    "\n",
    "X = torch.cat([feats_by_set['systematic'][t] for t in tasks]).numpy()\n",
    "all_gts = [torch.full_like(gt_by_set['systematic'][t], tidx) for tidx, t in enumerate(tasks)]\n",
    "all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "X_2d_clf = pca.transform(clf_vectors)\n",
    "# X_2d_clf = pca.transform(word_vectors)\n",
    "\n",
    "scatter_pca(X_2d,\n",
    "            all_gts,\n",
    "            labels_to_use=labels_to_use,\n",
    "            special_X=X_2d_clf,\n",
    "            special_y=special_gts,\n",
    "            title='Systematic Test',\n",
    "            don_t_label_these=don_t_label_these,\n",
    "            ax=axs[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0bb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab000c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e5bfbe4",
   "metadata": {},
   "source": [
    "### Concept Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc80477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eefe6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "labels_to_use = tasks\n",
    "\n",
    "# X = torch.cat([feats_by_set['test'][t] for t in tasks]).numpy()\n",
    "# all_gts = [torch.full_like(gt_by_set['test'][t], tidx) for tidx, t in enumerate(tasks)]\n",
    "# all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "\n",
    "X = torch.cat([clf_vectors_by_task[t] for t in tasks] + \n",
    "              [word_vectors_by_task[t] for t in tasks]).numpy()\n",
    "\n",
    "clf_gts = [torch.full_like(clf_idxs_by_task[t], tidx) for tidx, t in enumerate(tasks)]\n",
    "word_gts = [torch.full_like(clf_idxs_by_task[t], tidx) for tidx, t in enumerate(tasks)]\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "# _ = pca.fit_transform(X)\n",
    "X_2d_word = pca.fit_transform(word_vectors)\n",
    "word_gts = torch.cat(word_gts).numpy()\n",
    "\n",
    "X_2d_clf = pca.transform(clf_vectors)\n",
    "clf_gts = torch.cat(clf_gts).numpy()\n",
    "\n",
    "scatter_pca(X_2d_word,\n",
    "            word_gts,\n",
    "            labels_to_use=labels_to_use,\n",
    "            title=f'Concept Embedding for {n_colors} Color Dataset',\n",
    "            don_t_label_these=don_t_label_these,\n",
    "#             special_X=X_2d_clf,\n",
    "#             special_y=special_gts,\n",
    "#             ax=axs[0]\n",
    "           )\n",
    "\n",
    "# scatter_pca(X_2d_clf,\n",
    "#             clf_gts,\n",
    "#             labels_to_use=labels_to_use,\n",
    "#             title='Classifier',\n",
    "#             don_t_label_these=don_t_label_these,\n",
    "# #             special_X=X_2d_clf,\n",
    "# #             special_y=special_gts,\n",
    "#             ax=axs[1]\n",
    "#            )\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ccb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c64a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "# labels_to_use = tasks\n",
    "\n",
    "# X = torch.cat([feats_by_set['test'][t] for t in tasks]).numpy()\n",
    "# all_gts = [torch.full_like(gt_by_set['test'][t], tidx) for tidx, t in enumerate(tasks)]\n",
    "# all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "# pca = PCA(n_components=3)\n",
    "# X_3d = pca.fit_transform(X)\n",
    "\n",
    "# special_X = torch.cat([clf_vectors_by_task[t] for t in tasks]).numpy()\n",
    "# special_gts = [torch.full_like(clf_idxs_by_task[t], tidx) for tidx, t in enumerate(tasks)]\n",
    "# special_gts = torch.cat(special_gts).numpy()\n",
    "# X_3d_clf = pca.transform(clf_vectors)\n",
    "\n",
    "# scatter_pca(X_3d,\n",
    "#             all_gts,\n",
    "#             labels_to_use=labels_to_use,\n",
    "#             special_X=X_3d_clf,\n",
    "#             special_y=special_gts,\n",
    "#             title='IID Test',\n",
    "#             don_t_label_these=don_t_label_these,\n",
    "#             ax=axs[0])\n",
    "\n",
    "\n",
    "# X = torch.cat([feats_by_set['systematic'][t] for t in tasks]).numpy()\n",
    "# all_gts = [torch.full_like(gt_by_set['systematic'][t], tidx) for tidx, t in enumerate(tasks)]\n",
    "# all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# X_3d = pca.fit_transform(X)\n",
    "\n",
    "# X_3d_clf = pca.transform(clf_vectors)\n",
    "\n",
    "# scatter_pca(X_3d,\n",
    "#             all_gts,\n",
    "#             labels_to_use=labels_to_use,\n",
    "#             special_X=X_3d_clf,\n",
    "#             special_y=special_gts,\n",
    "#             title='Systematic Test',\n",
    "#             don_t_label_these=don_t_label_these,\n",
    "#             ax=axs[1])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f2dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b2d1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "labels_to_use = tasks\n",
    "\n",
    "X = torch.cat([feats_by_set['test'][t] for t in tasks]).numpy()\n",
    "all_gts = torch.cat([gt_by_set['test'][t] for t in tasks]).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "special_X = torch.cat([clf_vectors_by_task[t] for t in tasks]).numpy()\n",
    "special_gts = torch.cat([clf_idxs_by_task[t] for t in tasks]).numpy()\n",
    "X_2d_clf = pca.transform(clf_vectors)\n",
    "\n",
    "scatter_pca(X_2d,\n",
    "            all_gts,\n",
    "#             labels_to_use=labels_to_use,\n",
    "            special_X=X_2d_clf,\n",
    "            special_y=special_gts,\n",
    "            title='IID Test',\n",
    "            don_t_label_these=don_t_label_these,\n",
    "            ax=axs[0])\n",
    "\n",
    "\n",
    "X = torch.cat([feats_by_set['systematic'][t] for t in tasks]).numpy()\n",
    "all_gts = torch.cat([gt_by_set['systematic'][t] for t in tasks]).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "X_2d_clf = pca.transform(clf_vectors)\n",
    "\n",
    "scatter_pca(X_2d,\n",
    "            all_gts,\n",
    "#             labels_to_use=labels_to_use,\n",
    "            special_X=X_2d_clf,\n",
    "            special_y=special_gts,\n",
    "            title='Systematic Test',\n",
    "            don_t_label_these=don_t_label_these,\n",
    "            ax=axs[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988cdb28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c95327a",
   "metadata": {},
   "source": [
    "### Color + Shape Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc34f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "labels_to_use = ['colors', 'shapes']\n",
    "\n",
    "X = torch.cat([feats_by_set['test'][t] for t in ['colors', 'shapes']]).numpy()\n",
    "all_gts = [torch.full_like(gt_by_set['test'][t], tidx) for tidx, t in enumerate(['colors', 'shapes'])]\n",
    "all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(\n",
    "    X_2d, \n",
    "    all_gts, \n",
    "    labels_to_use=labels_to_use, \n",
    "    title='IID Test',\n",
    "    don_t_label_these=don_t_label_these,\n",
    "    ax=axs[0])\n",
    "\n",
    "\n",
    "X = torch.cat([feats_by_set['systematic'][t] for t in ['colors', 'shapes']]).numpy()\n",
    "all_gts = [torch.full_like(gt_by_set['systematic'][t], tidx) for tidx, t in enumerate(['colors', 'shapes'])]\n",
    "all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(\n",
    "    X_2d,\n",
    "    all_gts,\n",
    "    labels_to_use=labels_to_use,\n",
    "    title='Systematic Test',\n",
    "    don_t_label_these=don_t_label_these,\n",
    "    ax=axs[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b6b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "X = torch.cat([feats_by_set['test'][t] for t in ['colors', 'shapes']]).numpy()\n",
    "all_gts = torch.cat([gt_by_set['test'][t] for t in ['colors', 'shapes']]).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, \n",
    "            all_gts, \n",
    "            don_t_label_these=don_t_label_these,\n",
    "            title='IID Test', \n",
    "            ax=axs[0])\n",
    "\n",
    "\n",
    "X = torch.cat([feats_by_set['systematic'][t] for t in ['colors', 'shapes']]).numpy()\n",
    "all_gts = torch.cat([gt_by_set['systematic'][t] for t in ['colors', 'shapes']]).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, \n",
    "            all_gts, \n",
    "            don_t_label_these=don_t_label_these, \n",
    "            title='Systematic Test', \n",
    "            ax=axs[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549b400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4732103c",
   "metadata": {},
   "source": [
    "### Color Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565dd634",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa652059",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_X = torch.cat([clf_vectors_by_task[t] for t in ['colors']]).numpy()\n",
    "special_gts = torch.cat([clf_idxs_by_task[t] for t in ['colors']]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "X = feats_by_set['test']['colors'].numpy()\n",
    "all_gts = gt_by_set['test']['colors'].numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, all_gts, don_t_label_these=don_t_label_these, title='IID Test', ax=axs[0])\n",
    "\n",
    "\n",
    "X = feats_by_set['systematic']['colors'].numpy()\n",
    "all_gts = gt_by_set['systematic']['colors'].numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, all_gts, don_t_label_these=don_t_label_these, title='Systematic Test', ax=axs[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ace793",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daccc4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat([feats_by_set[set_]['colors'] for set_ in ['test', 'systematic']]).numpy()\n",
    "all_gts = [torch.full_like(gt_by_set[set_]['colors'], tidx) for tidx, set_ in enumerate(['test', 'systematic'])]\n",
    "all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, all_gts, labels_to_use=['test', 'systematic'], don_t_label_these=[], title='Systematic Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743700f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.cat([feats_by_set[set_]['colors'] for set_ in ['test', 'systematic']]).numpy()\n",
    "# all_gts = [torch.full_like(gt_by_set[set_]['colors'], tidx) for tidx, set_ in enumerate(['test', 'systematic'])]\n",
    "# all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "# pca = PCA(n_components=3)\n",
    "# X_3d = pca.fit_transform(X)\n",
    "\n",
    "# scatter_pca(X_3d, all_gts, labels_to_use=['test', 'systematic'], don_t_label_these=[], title='Systematic Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ea083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.cat([feats_by_set[set_]['colors'] for set_ in ['test', 'systematic']]).numpy()\n",
    "# all_gts = [gt_by_set[set_]['colors'] for set_ in ['test', 'systematic']]\n",
    "# all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "# pca = PCA(n_components=3)\n",
    "# X_3d = pca.fit_transform(X)\n",
    "\n",
    "# scatter_pca(X_3d,\n",
    "#             all_gts,\n",
    "# #             labels_to_use=['test', 'systematic'],\n",
    "#             don_t_label_these=don_t_label_these,\n",
    "#             title='Systematic Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f468366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f6a9c78",
   "metadata": {},
   "source": [
    "### Shape Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0946d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8595edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_X = torch.cat([clf_vectors_by_task[t] for t in ['shapes']]).numpy()\n",
    "special_gts = torch.cat([clf_idxs_by_task[t] for t in ['shapes']]).numpy()\n",
    "\n",
    "\n",
    "# special_X = torch.cat([clf_vectors_by_task[t] for t in tasks]).numpy()\n",
    "# special_gts = torch.cat([clf_idxs_by_task[t] for t in tasks]).numpy()\n",
    "# X_2d_clf = pca.transform(special_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fae608",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "X = feats_by_set['test']['shapes'].numpy()\n",
    "all_gts = gt_by_set['test']['shapes'].numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "scatter_pca(\n",
    "    X_2d, \n",
    "    all_gts, \n",
    "    special_X=pca.transform(special_X),\n",
    "    special_y=special_gts,\n",
    "    don_t_label_these=[], \n",
    "    title='IID Test', \n",
    "    ax=axs[0])\n",
    "\n",
    "\n",
    "X = feats_by_set['systematic']['shapes'].numpy()\n",
    "all_gts = gt_by_set['systematic']['shapes'].numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(\n",
    "    X_2d, \n",
    "    all_gts, \n",
    "    special_X=pca.transform(special_X),\n",
    "    special_y=special_gts,\n",
    "    don_t_label_these=[], \n",
    "    title='Systematic Test', \n",
    "    ax=axs[1])\n",
    "\n",
    "# plt.savefig(\n",
    "#     'exports/acc-color-smaller-models.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664718b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69792fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = torch.cat([feats_by_set[set_]['shapes'] for set_ in ['test', 'systematic']]).numpy()\n",
    "all_gts = [torch.full_like(gt_by_set[set_]['shapes'], tidx) for tidx, set_ in enumerate(['test', 'systematic'])]\n",
    "all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, \n",
    "            all_gts,\n",
    "            labels_to_use=['test', 'systematic'], \n",
    "            special_X=pca.transform(special_X),\n",
    "            special_y=special_gts,\n",
    "            special_labels_to_use=processor.inv_vocabulary, \n",
    "            don_t_label_these=[], \n",
    "            title='Test and Systematic Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f44b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = torch.cat([feats_by_set[set_]['shapes'] for set_ in ['test', 'systematic']]).numpy()\n",
    "all_gts = [gt_by_set[set_]['shapes'] for set_ in ['test', 'systematic']]\n",
    "all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, \n",
    "            all_gts,\n",
    "#             labels_to_use=['test', 'systematic'], \n",
    "            special_X=pca.transform(special_X),\n",
    "            special_y=special_gts,\n",
    "            special_labels_to_use=processor.inv_vocabulary, \n",
    "            don_t_label_these=[], \n",
    "            title='Test and Systematic Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b32b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gts.shape, colors.shape, sizes.shape, materials.shape, shapes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7263a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "X = feats_by_set['test']['shapes'].numpy()\n",
    "# all_gts = gt_by_set['test']['shapes'].numpy()\n",
    "sizes, colors, materials, shapes = props_by_set['test']['shapes']\n",
    "all_gts = colors.numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "scatter_pca(\n",
    "    X_2d, \n",
    "    all_gts, \n",
    "    special_X=pca.transform(special_X),\n",
    "    special_y=special_gts,\n",
    "    don_t_label_these=don_t_label_these, \n",
    "    title='IID Test', \n",
    "    ax=axs[0])\n",
    "\n",
    "\n",
    "X = feats_by_set['systematic']['shapes'].numpy()\n",
    "# all_gts = gt_by_set['systematic']['shapes'].numpy()\n",
    "sizes, colors, materials, shapes = props_by_set['systematic']['shapes']\n",
    "all_gts = colors.numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(\n",
    "    X_2d, \n",
    "    all_gts, \n",
    "    special_X=pca.transform(special_X),\n",
    "    special_y=special_gts,\n",
    "    don_t_label_these=don_t_label_these,\n",
    "    title='Systematic Test',\n",
    "    ax=axs[1])\n",
    "\n",
    "# plt.savefig(\n",
    "#     'exports/acc-color-smaller-models.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b16855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64082391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.cat([feats_by_set[set_]['shapes'] for set_ in ['test', 'systematic']]).numpy()\n",
    "# all_gts = [torch.full_like(gt_by_set[set_]['shapes'], tidx) for tidx, set_ in enumerate(['test', 'systematic'])]\n",
    "# all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "# pca = PCA(n_components=3)\n",
    "# X_3d = pca.fit_transform(X)\n",
    "\n",
    "# scatter_pca(X_3d,\n",
    "#             all_gts,\n",
    "#             labels_to_use=['test', 'systematic'],\n",
    "#             special_X=pca.transform(special_X),\n",
    "#             special_y=special_gts,\n",
    "#             special_labels_to_use=processor.inv_vocabulary,\n",
    "#             don_t_label_these=[],\n",
    "#             title='Test and Systematic Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a020277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a122fa65",
   "metadata": {},
   "source": [
    "### Material Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5395a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "X = feats_by_set['test']['materials'].numpy()\n",
    "all_gts = gt_by_set['test']['materials'].numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, all_gts, don_t_label_these=[], title='IID Test', ax=axs[0])\n",
    "\n",
    "\n",
    "X = feats_by_set['systematic']['materials'].numpy()\n",
    "all_gts = gt_by_set['systematic']['materials'].numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, all_gts, don_t_label_these=[], title='Systematic Test', ax=axs[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a347b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66652014",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat([feats_by_set[set_]['materials'] for set_ in ['test', 'systematic']]).numpy()\n",
    "all_gts = [torch.full_like(gt_by_set[set_]['materials'], tidx) for tidx, set_ in enumerate(['test', 'systematic'])]\n",
    "all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, all_gts, labels_to_use=['test', 'systematic'], don_t_label_these=[], title='Systematic Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537c322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "776501f3",
   "metadata": {},
   "source": [
    "### Size Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(1.5*9*1.75,5*1.75))\n",
    "\n",
    "X = feats_by_set['test']['sizes'].numpy()\n",
    "all_gts = gt_by_set['test']['sizes'].numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, all_gts, don_t_label_these=[], title='IID Test', ax=axs[0])\n",
    "\n",
    "\n",
    "X = feats_by_set['systematic']['sizes'].numpy()\n",
    "all_gts = gt_by_set['systematic']['sizes'].numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, all_gts, don_t_label_these=[], title='Systematic Test', ax=axs[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee98e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.cat([feats_by_set[set_]['sizes'] for set_ in ['test', 'systematic']]).numpy()\n",
    "all_gts = [torch.full_like(gt_by_set[set_]['sizes'], tidx) for tidx, set_ in enumerate(['test', 'systematic'])]\n",
    "all_gts = torch.cat(all_gts).numpy()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "scatter_pca(X_2d, all_gts, labels_to_use=['test', 'systematic'], don_t_label_these=[], title='Systematic Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4487058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cd90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb92cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

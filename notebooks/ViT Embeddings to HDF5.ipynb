{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9daeb206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50d72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08fe3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = torch.load('/mnt/ialabnas/datasets/CLEVR_CoGenT_v1.0/images/valA-vit.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7debd2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image_name_to_idx', 'image_features'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22c8352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15000, 197, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats['image_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e4332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3851ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# from model import ViTEmbedding\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ImageDataset:\n",
    "    def __init__(self, images_dir, split):\n",
    "        self.images_dir = images_dir\n",
    "        self.image_list = list(Path(images_dir).glob('*.png'))\n",
    "        self.image_list = sorted(self.image_list,\n",
    "                                 key=lambda x: int(x.name.replace('.png','').replace(f'CLEVR_{split}_','')))\n",
    "        self.transforms = transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_list[idx]\n",
    "        image_tensor = self.transforms(self.load_image(image_path))\n",
    "        return image_tensor\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        return Image.open(image_path).convert('RGB')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efea756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/workspace1/fidelrio/CLEVR_CoGenT_v1.0/'\n",
    "base_dir2 = '/mnt/ialabnas/datasets/CLEVR_CoGenT_v1.0/'\n",
    "    \n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c68105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = 'trainA'\n",
    "# base_dir = '/workspace1/fidelrio/CLEVR_CoGenT_v1.0/'\n",
    "# featues_path = f'{base_dir}/images/{split}-vit.pt'\n",
    "\n",
    "# features_data = torch.load(featues_path)\n",
    "# features = features_data['image_features']\n",
    "# image_name_to_idx = features_data['image_name_to_idx']\n",
    "\n",
    "# features = features.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eda5c60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# from models.load_pretrained_models import load_model\n",
    "# from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from model import ViTEmbedding\n",
    "\n",
    "# model = load_model('resnet50_trained_on_SIN_and_IN_then_finetuned_on_IN').to(device)\n",
    "\n",
    "# feature_maps = []  # This will be a list of Tensors, each representing a feature map\n",
    "\n",
    "# def hook_feat_map(mod, inp, out):\n",
    "#     feature_maps.append(out.flatten(2).transpose(1,2).detach().cpu())\n",
    "\n",
    "# model.module.layer4.register_forward_hook(hook_feat_map)\n",
    "\n",
    "\n",
    "model = ViTEmbedding().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89723a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'valA'\n",
    "images_dir = f'{base_dir}/images/{split}'\n",
    "dataset = ImageDataset(images_dir, split)\n",
    "\n",
    "# dataset.transforms = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dda647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f6e4c7ccd24a429306f02dbcf69e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2188 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split in ['trainA', 'valA', 'valB', 'testA', 'testB']:\n",
    "    images_dir = f'{base_dir}/images/{split}'\n",
    "    dataset = ImageDataset(images_dir, split)\n",
    "\n",
    "    num_workers = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", 4))\n",
    "    loader = DataLoader(dataset, batch_size=32, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    all_outs = []\n",
    "    for images in tqdm(loader):\n",
    "        images = images.to(device)\n",
    "        all_outs.append(model(images).detach().cpu())\n",
    "\n",
    "    features = torch.cat(all_outs)\n",
    "\n",
    "#     features = torch.cat(feature_maps)\n",
    "#     feature_maps.clear()\n",
    "    features = features.numpy()\n",
    "    image_name_to_idx = {image_path.name: idx for idx, image_path in enumerate(dataset.image_list)}\n",
    "    \n",
    "    with h5py.File(f'{base_dir2}/images/{split}-vit.h5', 'w') as ds:\n",
    "        ds.create_dataset(\"image_features\", data=features)\n",
    "        ds[\"image_features\"].attrs[\"image_name_to_idx\"] = json.dumps(image_name_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26643bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7447db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52615f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae9f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
